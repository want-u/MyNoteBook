# K8s-健康检查

## 1. Health Check

强大的自愈能力是 Kubernetes 这类容器编排引擎的一个重要特性。自愈的默认实现方式是自动重启发生故障的容器。除此之外，用户还可以利用 Liveness 和 Readiness 探测机制设置更精细的健康检查，进而实现如下需求：

1. 零停机部署。
2. 避免部署无效的镜像。
3. 更加安全的滚动升级。

## 2. 默认健康检查

每个容器启动时都会执行一个进程，此进程由 Dockerfile 的 CMD 或 ENTRYPOINT 指定。

如果进程退出时返回码非零，则认为容器发生故障，Kubernetes 就会根据 restartPolicy 重启容器。

```
[root@k8s-master ~]# kubectl run healthcheck --image=busybox --dry-run="client" -o yaml > healthcheck.yaml

[root@k8s-master ~]# cat healthcheck.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: healthcheck
  name: healthcheck
spec:
  containers:
  - image: busybox
    name: healthcheck
    args: ["/bin/sh","-c","sleep 10; exit 1"]
  restartPolicy: OnFailure
[root@k8s-master ~]# kubectl apply -f healthcheck.yaml 

[root@k8s-master ~]# kubectl get pod -o wide 
NAME          READY   STATUS              RESTARTS   AGE   IP       NODE        NOMINATED NODE   READINESS GATES
healthcheck   0/1     ContainerCreating   0          6s    <none>   k8s-node2   <none>           <none>

[root@k8s-master ~]# watch kubectl get pod -o wide
# 查看异常退出和重启的状态
Every 99.0s: kubectl get pod -o wide                                    Sat Mar 20 14:20:05 2021

NAME          READY   STATUS             RESTARTS   AGE   IP            NODE        NOMINATED NO
DE   READINESS GATES
healthcheck   0/1     CrashLoopBackOff   2          2m    10.244.2.83   k8s-node2   <none>
     <none>

```

## 3. Liveness探测

Liveness 探测让用户可以自定义判断容器是否健康的条件。

如果探测失败，Kubernetes 就会重启容器。

```
[root@k8s-master ~]# cat livenesspod.yml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: healthcheck
  name: healthcheck
spec:
  restartPolicy: OnFailure
  containers:
  - image: busybox
    name: healthcheck
    args: ["/bin/sh","-c","touch /tmp/healty; sleep 30; rm -f /tmp/healty; sleep 99999"]
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healty
      initialDelaySeconds: 10
      periodSeconds: 5

# 应用yaml文件
[root@k8s-master ~]# kubectl apply -f livenesspod.yml
# 观察容器状态[重启次数]
[root@k8s-master ~]# watch kubectl get pod -o wide
# 观察事件过程
[root@k8s-master ~]# kubectl describe pod healthcheck
# 在线修改参数配置
[root@k8s-master ~]# kubectl edit pod healthcheck
# 删除pod
[root@k8s-master ~]# kubectl delete pod healthcheck
```

## 4. readiness探测

用户通过 Liveness 探测可以告诉 Kubernetes 什么时候通过重启容器实现自愈；

Readiness 探测则是告诉 Kubernetes 什么时候可以将容器加入到 Service 负载均衡池中，对外提供服务。

Readiness 探测的配置语法与 Liveness 探测完全一样，只是将 liveness 替换为了 readiness

```
[root@k8s-master ~]# cat readinesspod.yml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: healthcheck
  name: healthcheck
spec:
  restartPolicy: OnFailure
  containers:
  - image: busybox
    name: healthcheck
    args: ["/bin/sh","-c","touch /tmp/healty; sleep 30; rm -f /tmp/healty; sleep 99999"]
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/healty
      initialDelaySeconds: 10
      periodSeconds: 5
      
[root@k8s-master ~]# kubectl apply -f readinesspod.yml 
# 观察pod状态  
[root@k8s-master ~]# kubectl describe pod healthcheck
```

## 5. 健康检查场景

### 5.1 在Scale Up 中使用Health Check

对于多副本应用，当执行 Scale Up 操作时，新副本会作为 backend 被添加到 Service 的负责均衡中，与已有副本一起处理客户的请求。

考虑到应用启动通常都需要一个准备阶段，比如加载缓存数据，连接数据库等，从容器启动到正真能够提供服务是需要一段时间的。

我们可以通过 Readiness 探测判断容器是否就绪，避免将请求发送到还没有 ready 的 backend。

```
# 创建deployment
[root@k8s-master ~]# kubectl create deployment web --image=httpd --dry-run="client" -o yaml > web.yml
[root@k8s-master ~]# cat web.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: web
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - image: httpd
        name: httpd
        ports:
        -containerPort: 80
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /healthy
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5

```

```
# 创建service
[root@k8s-master ~]# kubectl create service nodeport web-svc --tcp=8080:80 --dry-run="client" -o yaml >> web.yml 
[root@k8s-master ~]# vim web.yml 
[root@k8s-master ~]# cat web.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: web
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - image: httpd
        name: httpd
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /healthy
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: web-svc
  name: web-svc
spec:
  ports:
  - name: 8080-80
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: web
  type: NodePort
  
# 查看副本状态  
[root@k8s-master ~]# kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
web    0/3     3            0           12s
[root@k8s-master ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          12d
web-svc      NodePort    10.104.193.9   <none>        8080:31427/TCP   17s  
# 查看pod状态
[root@k8s-master ~]# kubectl get pod -o wide 
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE        NOMINATED NODE   READINESS GATES
web-675d48dcfb-4t6qz   0/1     Running   0          66s   10.244.1.78   k8s-node1   <none>           <none>
web-675d48dcfb-knzhz   0/1     Running   0          66s   10.244.1.79   k8s-node1   <none>           <none>
web-675d48dcfb-ngnjn   0/1     Running   0          66s   10.244.2.85   k8s-node2   <none>           <none>
[root@k8s-master ~]# curl 10.104.193.9:8080
curl: (7) Failed connect to 10.104.193.9:8080; 拒绝连接
[root@k8s-master ~]# curl k8s-master:31427
curl: (7) Failed connect to k8s-master:31427; 拒绝连接
# 查看pod事件
[root@k8s-master ~]# kubectl describe pod web-675d48dcfb-ngnjn

# 当删除健康探测后，副本可以正常运行
```

### 5.2 滚动更新的健康检查

如果正确配置了 Health Check，新副本只有通过了 Readiness 探测，才会被添加到 Service；

如果没有通过探测，现有副本不会被全部替换，业务仍然正常进行。

```
# 创建deployment
# 使用args创建一个文件
# 使用readinessProbe做健康检查
[root@k8s-master ~]# kubectl create deployment test1 --image=busybox --dry-run="client" -o yaml > test1.yaml
[root@k8s-master ~]# vim test1.yaml 
[root@k8s-master ~]# cat test1.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test1
  name: test1
spec:
  replicas: 10
  selector:
    matchLabels:
      app: test1
  template:
    metadata:
      labels:
        app: test1
    spec:
      containers:
      - image: busybox
        name: busybox
        args: ["/bin/sh","-c","sleep 10; touch /tmp/healthy; sleep 999999"]
        readinessProbe:
          exec:
            command: ["cat","/tmp/healthy"]
          initialDelaySeconds: 10
          periodSeconds: 5

# 应用yaml文件
[root@k8s-master ~]# kubectl apply -f test1.yaml
[root@k8s-master ~]# kubectl get pod 
NAME                     READY   STATUS    RESTARTS   AGE
test1-5cf6cc6958-4mw4p   1/1     Running   0          80s
test1-5cf6cc6958-6h42j   1/1     Running   0          80s
test1-5cf6cc6958-6mpcb   1/1     Running   0          80s
test1-5cf6cc6958-7qkvx   1/1     Running   0          80s
test1-5cf6cc6958-965hw   1/1     Running   0          80s
test1-5cf6cc6958-jt8mc   1/1     Running   0          80s
test1-5cf6cc6958-jvmjg   1/1     Running   0          80s
test1-5cf6cc6958-mn4pd   1/1     Running   0          80s
test1-5cf6cc6958-mn9mt   1/1     Running   0          80s
test1-5cf6cc6958-wfqf8   1/1     Running   0          80s

```

```
# 更新测试
# 删除创建文件的参数，模拟一个滚动更新失败的场景

# Health Check 会帮我们屏蔽了有缺陷的副本，同时保留了大部分旧副本，业务没有因更新失败受到影响

[root@k8s-master ~]# cp test1.yaml test2.yaml 
[root@k8s-master ~]# vim test2.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test1
  name: test1
spec:
  replicas: 10
  selector:
    matchLabels:
      app: test1
  template:
    metadata:
      labels:
        app: test1
    spec:
      containers:
      - image: busybox
        name: busybox
        args: ["/bin/sh","-c","sleep 999999"]
        readinessProbe:
          exec:
            command: ["cat","/tmp/healthy"]
          initialDelaySeconds: 10
          periodSeconds: 5

[root@k8s-master ~]# kubectl apply -f test2.yaml --record
[root@k8s-master ~]# kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
test1-5cf6cc6958-4mw4p   1/1     Running   0          4m38s   10.244.2.91   k8s-node2   <none>           <none>
test1-5cf6cc6958-6h42j   1/1     Running   0          4m38s   10.244.1.83   k8s-node1   <none>           <none>
test1-5cf6cc6958-6mpcb   1/1     Running   0          4m38s   10.244.2.92   k8s-node2   <none>           <none>
test1-5cf6cc6958-7qkvx   1/1     Running   0          4m38s   10.244.1.87   k8s-node1   <none>           <none>
test1-5cf6cc6958-965hw   1/1     Running   0          4m38s   10.244.2.90   k8s-node2   <none>           <none>
test1-5cf6cc6958-jt8mc   1/1     Running   0          4m38s   10.244.1.84   k8s-node1   <none>           <none>
test1-5cf6cc6958-mn4pd   1/1     Running   0          4m38s   10.244.1.85   k8s-node1   <none>           <none>
test1-5cf6cc6958-mn9mt   1/1     Running   0          4m38s   10.244.1.86   k8s-node1   <none>           <none>
test1-8688f995fc-2xpfl   0/1     Running   0          99s     10.244.2.96   k8s-node2   <none>           <none>
test1-8688f995fc-6qgtd   0/1     Running   0          99s     10.244.1.88   k8s-node1   <none>           <none>
test1-8688f995fc-d8llr   0/1     Running   0          98s     10.244.1.89   k8s-node1   <none>           <none>
test1-8688f995fc-vn2kr   0/1     Running   0          98s     10.244.1.90   k8s-node1   <none>           <none>
test1-8688f995fc-zgpmq   0/1     Running   0          99s     10.244.2.95   k8s-node2   <none>           <none>

# 查看详情
# 滚动更新规则：RollingUpdateStrategy:  25% max unavailable, 25% max surge
[root@k8s-master ~]# kubectl describe deployments.apps test1 

```

```
# 查看回滚信息
[root@k8s-master ~]# kubectl rollout history deployment test1 
deployment.apps/test1 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl apply --filename=test2.yaml --record=true

# 回滚
[root@k8s-master ~]# kubectl rollout undo deployment test1 -to-revision=1
[root@k8s-master ~]# kubectl get pod
NAME                     READY   STATUS        RESTARTS   AGE
test1-5cf6cc6958-4mw4p   1/1     Running       0          13m
test1-5cf6cc6958-6h42j   1/1     Running       0          13m
test1-5cf6cc6958-6mpcb   1/1     Running       0          13m
test1-5cf6cc6958-7qkvx   1/1     Running       0          13m
test1-5cf6cc6958-965hw   1/1     Running       0          13m
test1-5cf6cc6958-jt8mc   1/1     Running       0          13m
test1-5cf6cc6958-jxb47   1/1     Running       0          20s
test1-5cf6cc6958-mn4pd   1/1     Running       0          13m
test1-5cf6cc6958-mn9mt   1/1     Running       0          13m
test1-5cf6cc6958-vqds8   1/1     Running       0          20s
test1-8688f995fc-dvcfp   0/1     Terminating   0          3m58s
test1-8688f995fc-fppfc   0/1     Terminating   0          3m58s
test1-8688f995fc-nnmzb   0/1     Terminating   0          3m58s
test1-8688f995fc-r2brh   0/1     Terminating   0          3m58s
test1-8688f995fc-txqn9   0/1     Terminating   0          3m58s
```

### 5.3 调整滚动更新规则

滚动更新通过参数 maxSurge 和 maxUnavailable 来控制副本替换的数量

maxSurge

- 此参数控制滚动更新过程中副本总数的超过 DESIRED 的上限。maxSurge 可以是具体的整数（比如 3），也可以是百分百，向上取整。maxSurge 默认值为 25%。

maxUnavailable

- 此参数控制滚动更新过程中，不可用的副本相占 DESIRED 的最大比例。 maxUnavailable 可以是具体的整数（比如 3），也可以是百分百，向下取整。maxUnavailable 默认值为 25%。

```
[root@k8s-master ~]# cat test2.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test1
  name: test1
spec:
  strategy:
    rollingUpdate:
      maxSurge: 35%
      maxUnavailable: 35%
  replicas: 10
  selector:
    matchLabels:
      app: test1
  template:
    metadata:
      labels:
        app: test1
    spec:
      containers:
      - image: busybox
        name: busybox
        args: ["/bin/sh","-c","sleep 999999"]
        readinessProbe:
          exec:
            command: ["cat","/tmp/healthy"]
          initialDelaySeconds: 10
          periodSeconds: 5

[root@k8s-master ~]# kubectl apply -f test2.yaml --record

[root@k8s-master ~]# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
test1-5cf6cc6958-4mw4p   1/1     Running   0          17m
test1-5cf6cc6958-6h42j   1/1     Running   0          17m
test1-5cf6cc6958-6mpcb   1/1     Running   0          17m
test1-5cf6cc6958-965hw   1/1     Running   0          17m
test1-5cf6cc6958-jt8mc   1/1     Running   0          17m
test1-5cf6cc6958-mn4pd   1/1     Running   0          17m
test1-5cf6cc6958-mn9mt   1/1     Running   0          17m
test1-8688f995fc-25r5j   0/1     Running   0          65s
test1-8688f995fc-9jthq   0/1     Running   0          64s
test1-8688f995fc-dtxkq   0/1     Running   0          64s
test1-8688f995fc-j9vz8   0/1     Running   0          65s
test1-8688f995fc-kcmwh   0/1     Running   0          65s
test1-8688f995fc-plq4d   0/1     Running   0          65s
test1-8688f995fc-q8nrr   0/1     Running   0          64s

```

