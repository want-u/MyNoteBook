# Docker跨网络

Docker跨网络的三种方案：

- overlay
- macvxlan
- weave

## 1. overlay

Vxlan底层技术，Vxlan有一个功能，它把二层的数据封装到UDP里传输，跟 connect有点像，诵过添加网卡的方法，内网一张网卡，外网一张网卡。

consul：数据中心的含义，可以将其当做数据库来理解，类似于Redis等非关系型数据库，采用的是键-值对的方式，存放着各个容器的IP及端口信息。

consul的功能很强大，可以以群集的方式运行，并且具备健康监测等功能。

需要有一个键值对，来保存我们的网络状态信息，比如 consul、etcd、zookeeper

### 1.1 部署环境

|     主机     |      ip       |
| :----------: | :-----------: |
|    consul    | 192.168.10.11 |
| docker-node1 | 192.168.10.12 |
| docker-node2 | 192.168.10.13 |

### 1.2 下载镜像

```
docker search  consul
docker pull  progrium/consul
```

### 1.3 开启防火墙规则

每台都要做

```
firewall-cmd --add-port={7946,4789,2733,2376}/tcp --permanent
firewall-cmd --add-port={7946,4789,2733,2376}/udp --permanent
firewall-cmd --reload
```

### 1.4 刷新内核参数

```
[root@server1 ~]# sysctl -a | grep bridge-nf-call
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

[root@server1 ~]# sysctl -a | grep ip_forward
net.ipv4.ip_forward = 1
```

### 1.5 启动consul容器

```
docker run -d --restart always  -p 8400:8400 -p 8500:8500 -p 8600:53/udp  -h node1 progrium/consul -server -bootstrap -ui-dir /ui
```

访问页面：http://192.168.10.11:8500

### 1.6 指定consul服务器

在node1和node2上修改

```
[root@host1 ~]# vim /etc/docker/daemon.json
# 文件内容
{
"registry-mirrors":["https://8p2ftmks.mirror.aliyuncs.com"],
"cluster-store": "consul://192.168.10.11:8500",
"cluster-advertise": "ens33:2376"
}

[root@host1 ~]# systemctl daemon-reload && systemctl restart docker
```

### 1.7 创建overlay网络

在docker-node1上创建一个overlay网络，之后去docker-node2查看网络

```
docker network  create -d overlay  ov_net1
docker network ls
docker network  inspect ov_net1
```

### 1.8 启动容器

在两个节点创建以overlay网络生成的容器，验证网络

```
# docker-node1
docker run -dit --name test1 --network ov_net1 busybox
# docker-node2
docker run -dit --name test2 --network ov_net1 busybox
```

```
# docker-node1
[root@host1 ~]# docker exec -it test1 sh
/ # ip a
/ # ping 10.0.0.3
```

## 2. macvxlan

MacVlan工作原理：

MacVlan是Linux内核支持的网络接口。要求的Linux内部版本是v3.9-3.19和4.0+。

通过物理网卡创建MacVlan子接口，允许一块物理网卡拥有多个独立的MAC地址和IP地址。虚拟的子接口将直接暴露在相邻物理网络中。

当发送包时。物理网卡收到包后，会根据收到包的目的MAC地址判断这个包需要交给哪个其中的虚拟网卡。

本质上首先时宿主机物理网卡工作在“混杂模式”，这样物理网卡的MAC地址将会失效，所有二层网络中的流量物理网卡都能收到。接下来就是在这张物理网卡上创建虚拟网卡，并为虚拟网卡指定MAC地址，实现一卡多用，在物理网络看来，每张虚拟网卡都是一个单独的接口。

### 2.1 环境需求

|     主机     |      ip       |
| :----------: | :-----------: |
| docker-node1 | 192.168.10.12 |
| docker-node2 | 192.168.10.13 |

### 2.2 打开网卡混杂模式

node1和node2都需要开启混杂模式

```
ip link set ens33 promisc on
```

### 2.3 创建macvlan网络

```
# 在node1和node2上同时创建macvlan网络
# （两台节点应该是一样的属于同一个网络）
docker network create -d macvlan  --subnet 100.1.1.0/24 --gateway 100.1.1.1 -o parent=ens33 mac_net1
```

### 2.4 创建容器

```
docker run -dit --name box1 --ip 100.1.1.2 --network mac_net1 busybox
docker run -dit --name box2 --ip 100.1.1.4 --network mac_net1 busybox
```

```
# docker-node1
[root@host1 ~]# docker exec -it box1 sh
/ # ip a
/ # ping 100.1.1.4
```

### 2.5 vlan多接口

注意：

如果你是vmware环境的话，由于VMware虚拟机的原因，必须将两台主机默认的NAT模式修改为`桥接模式`才能够正常通信

| 主机  | ip (桥接模式) |
| :---: | :-----------: |
| host1 | 192.168.2.12  |
| host2 | 192.168.2.13  |

```
# 加载802.1q模块
modprobe 8021q
# 确认模块是否成功加载
[root@localhost ~]# lsmod |grep 8021q
8021q                  20475  0
garp                    7152  1 8021q

# 1. 首先分别在两台主机上将物理网口 enp0s8 创建出两个 VLAN 子接口
yum install -y vconfig

vconfig add ens33 100
vconfig add ens33 200
vconfig set_flag ens33.100 1 1
vconfig set_flag ens33.200 1 1
ifconfig ens33.100 up
ifconfig ens33.200 up
ip a

# 2. 分别在 host1 和 host2 上基于两个 VLAN 子接口创建 2 个 macvlan 网络，mac10 和 mac20
docker network create -d macvlan --subnet=172.16.10.0/24 --gateway=172.16.10.1 -o parent=ens33.100 mac10
docker network create -d macvlan --subnet=172.16.20.0/24 --gateway=172.16.20.1 -o parent=ens33.200 mac20
docker network ls

# 3. 分别在 host1 和 host2 上运行容器，并指定不同的 macvlan 网络。
# host1
docker run -itd --name d1 --ip=172.16.10.10 --network mac10 busybox
docker run -itd --name d2 --ip=172.16.20.10 --network mac20 busybox
 
# host2 
docker run -itd --name d3 --ip=172.16.10.11 --network mac10 busybox
docker run -itd --name d4 --ip=172.16.20.11 --network mac20 busybox

# 4. 验证
docker exec d1 ping -c 2 172.16.10.11
docker exec d1 ping -c 2 172.16.20.11
```

### 2.6 不同vlan互通

|  主机  | ip (桥接模式) |
| :----: | :-----------: |
| router | 192.168.2.11  |
| host1  | 192.168.2.12  |
| host2  | 192.168.2.13  |

通过验证，d1 和 d3，d2 和 d4 在同一 macvlan 网络下，互相可以 ping 通，d1 和 d2，d1 和 d4 在不同的 macvlan 网络下，互相 ping 不通。

这个原因也很明确，不同 macvlan 网络处于不同的网络，而且通过 VLAN 隔离，自然 ping 不了。

但这也只是在二层上通不了，通过三层的路由是可以通的，我们这就来验证下。

---

重新找一台主机 router，通过打开 `ip_forward` 把它改造成一台路由器，用来打通两个 macvlan 网络

1. 首先对 router 执行 `sysctl -w net.ipv4.ip_forward=1` 打开路由开关。

2. 然后创建两个 VLAN 子接口，一个作为 macvlan 网络 mac10 的网关，一个作为 mac20 的网关。

```
yum install -y vconfig

vconfig add ens33 100
vconfig add ens33 200
vconfig set_flag ens33.100 1 1
vconfig set_flag ens33.200 1 1
ifconfig ens33.100 up
ifconfig ens33.200 up
ip a

# 不同vlan互通须配置路由，对vlan子接口配置网关IP 并启用
ifconfig ens33.100 172.16.10.1 netmask 255.255.255.0 up
ifconfig ens33.200 172.16.20.1 netmask 255.255.255.0 up
```

3. 这样之后再从 d1 ping d2 和 d4，就可以 ping 通了。

---

PS：可能有些系统做了安全限制，可能 ping 不通，这时候可以添加以下 iptables 规则，目的是让系统能够转发不通 VLAN 的数据包。

```
iptables -t nat -A POSTROUTING -o ens33.100 -j MASQUERADE
iptables -t nat -A POSTROUTING -oens33.200 -j MASQUERADE
iptables -A FORWARD -i ens33.100 -o ens33.200 -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -i ens33.200 -o ens33.100 -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -i ens33.100 -o ens33.200 -j ACCEPT
iptables -A FORWARD -i ens33.200 -o ens33.100 -j ACCEPT
```

```
# 使用ip命令添加vlan
ip link add link ens33 name ens33.10 type vlan id 10
ip link set dev ens33.10 up
```

## 3. weave

官网：https://www.weave.works/docs/net/latest/install/installing-weave/

weave不依赖分布式数据库(例如etcd和consul)交换网络信息，每个主机上只需要运行weave组件就能建立起跨主机容器网络。

接下来在docker01和docker02上部署weave并实践weave的各项特性

首先配置好主机名：

```
hostnamectl set-hostname docker01
reboot
```

|   主机   |      ip       |
| :------: | :-----------: |
| docker01 | 192.168.10.11 |
| docker02 | 192.168.10.12 |

### 3.1 安装weave

在docker01和docker02上执行如下命令：

```
[root@docker01 ~]# curl -L git.io/weave -o /usr/local/bin/weave
[root@docker01 ~]# chmod a+x /usr/local/bin/weave

# 由于网络原因，将weave和weave.tar镜像包上传
rz 
# weave weave.tar
mv weave /usr/local/bin/weave && chmod a+x /usr/local/bin/weave
docker load -i weave.tar
```

### 3.2 启动weave

在docker01上启动weave：

注：在执行weave launch命令，启动weave相关服务。weave所有的组件都是以容器的方式运行的，会自动从docker hub上获取最新的image并启动容器

```
# 开启路由转发功能
[root@docker01 ~]# echo "1">/proc/sys/net/ipv4/ip_forward

[root@docker02 ~]# echo "1">/proc/sys/net/ipv4/ip_forward
```

```
# docker01启动weave
[root@docker01 ~] # weave launch
```

查看启动的容器：

weave是主程序，负责建立weave网络，收发数据，提供DNS服务等

```
[root@docker01 ~]# docker ps
CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS  NAMES
e089485df46f        weaveworks/weave:2.6.2   "/home/weave/weaver …"   55 seconds ago      Up 54 seconds              weave
```

```
# docker02启动weave，加入到同一个weave网络
# 格式 weave launch [docker01的ip地址]
[root@docker02 ~] # weave launch 192.168.10.11
```

### 3.3 查看状态

```
# 连接状态
[root@docker01 ~]# weave status connections
<- 192.168.10.12:42459   established fastdp e2:7e:49:7e:0f:1b(server1) mtu=1376

[root@docker02 ~]# weave status connections
-> 192.168.10.11:6783    established fastdp 06:0f:d4:4f:de:19(server1) mtu=1376
```

```
# 在两台主机上的docker 网络中都会多出weave 网络
[root@docker01 ~]# docker network list
NETWORK ID     NAME      DRIVER      SCOPE
9245a3d2601a   bridge    bridge      local
2664f5be1a1f   host      host        local
a20992b07474   none      null        local
ea6a6d1b88f1   weave     weavemesh   local
```

```
# 在一个节点上查看weave 环境之间建立的网路关系
[root@docker01 ~]# weave status peers
06:0f:d4:4f:de:19(docker01)
   <- 192.168.10.12:58699   e2:7e:49:7e:0f:1b(docker02)           established
e2:7e:49:7e:0f:1b(docker02)
   -> 192.168.10.11:6783    06:0f:d4:4f:de:19(docker01)           established
```

### 3.4 使用weave网络

```
# docker run -it --network weave busybox
```

### 3.5 attach附加

```
# docker01
[root@docker01 ~]# docker run -dit --name box1 busybox
# 使用attach向容器附加weave网络
# weave attach [ip] box1
[root@docker01 ~]# weave attach 10.0.0.11/24 box1

# docker02
[root@docker02 ~]# docker run -dit --name box2 busybox
[root@docker01 ~]# weave attach 10.0.0.12/24 box2

# 验证容器是否互通
[root@docker01 ~]# docker exec box1 ping -c 2 10.0.0.12

### 可以发现主机docker02上运行的容器可以与docker01上的容器通信
```

### 3.6 weave env

执行 eval \$(weave env) ，其作用是将后续的docker命令发送给weave proxy处理，使之后运行的容器都附加上weave网络；

如果要恢复之前的环境，可执行eval $(weave env --restore)。

```
# docker01
[root@docker01 ~]# eval $(weave env)
[root@docker01 ~]# docker run -dit --name box3 busybox

# docker02
[root@docker02 ~]# eval $(weave env)
[root@docker02 ~]# docker run -dit --name box4 busybox

# 验证容器是否互通
[root@docker01 ~]# docker exec -it box3 sh
/ # ip a
18: ethwe@if19: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue 
    link/ether 12:2c:e2:1e:04:c9 brd ff:ff:ff:ff:ff:ff
    inet 10.32.0.2/12 brd 10.47.255.255 scope global ethwe
       valid_lft forever preferred_lft forever
/ # ping  10.44.0.1       
```

 